# -*- coding: utf-8 -*-
"""cs-sample-calculate-metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PvCSBcuTKxB3C3gMGRVcQuVv7G1nI790
"""

import pandas as pd
import numpy as np
import re

from urllib.parse import unquote

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

import gensim
from gensim.models.keyedvectors import KeyedVectors
from gensim.models.fasttext import FastText
from gensim.models import word2vec

import pandas as pd
import numpy as np

import tensorflow as tf
import keras
from keras import backend as K
from keras.models import Sequential,Model,load_model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.constraints import maxnorm

import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

nltk.download('punkt')

# !sudo apt install tesseract-ocr

# !pip install pytesseract

# import pytesseract
# import shutil
# import os
# import random
# try:
#     from PIL import Image
# except ImportError:
#     import Image

from google.colab import drive
drive.mount('/content/drive')

folder_path = '/content/drive/MyDrive/LIRNEasia/ADB Project/'

def nltk_tokenizer(text):
    tokens = [word for word in word_tokenize(text)]
    stems = [PorterStemmer().stem(word) for word in tokens]
    return stems

embedding_size = 100
window_size = 40
min_word = 5
down_sampling = 1e-2
epochs=25

"""**# Reading Data**"""

tj_meta = pd.read_excel(folder_path+'data/metadata.xlsx') \
    .rename(columns={'job_code': 'tj_code', 'job_title': 'tj_title'}) \
    .drop(columns=['job_description', 'remark'])

tj_desc = pd.read_csv(folder_path+'data/ocr_sample_output.csv') \
    .rename(columns={'id': 'tj_code', 'cleaned_text': 'tj_desc'}) \
    .drop(columns=['Unnamed: 0', 'ocr_output', 'plain_accuracy'])

tj_desc.tj_code = tj_desc.tj_code.replace(
    to_replace='[^0-9]', value='', regex=True).astype(int)
tj_data = pd.merge(left=tj_meta, right=tj_desc, how='right', on='tj_code')

del tj_meta, tj_desc

onet_occ = pd.read_csv(folder_path+'data/onet_occ_titles.txt', sep='\t')

onet_alt = pd.read_csv(folder_path+'data/onet_alt_titles.txt', sep='\t') \
    .groupby(by='onet_code') \
    .agg({'onet_title_alt': lambda x: x.astype(object)}) \
    .reset_index()

onet_tech = pd.read_csv(folder_path+'data/onet_tech_skills.txt', sep='\t') \
    .groupby(by='onet_code') \
    .agg({'onet_tech': lambda x: x.astype(object)}) \
    .reset_index()

onet_data = pd.merge(left=onet_occ, right=onet_alt, how='left', on='onet_code')

onet_data = pd.merge(
    left=onet_data, right=onet_tech, how='left', on='onet_code') \
    .fillna('')

onet_data.to_csv(folder_path+'data/outputs/onet_data.csv', index=False)

del onet_occ, onet_alt, onet_tech

word_embedding_path= folder_path + "word embedding/fastText/"+"fastText"+str(embedding_size)
# cleaned_data_path = folder_path + 'Word embedding 2/poincare_train_corpus_100.csv'

"""**# Creating the Evaluation Corpus**"""

data_full = pd.read_excel(folder_path+'data/data_full.xlsx')

cs_image_path = folder_path+'data/cs_sample/'
cs_sample_tags = pd.read_csv(folder_path+'data/cs_sample_tags.csv')

cs_sample_tags['image_drive_url'] = cs_image_path + cs_sample_tags['file_name']
cs_sample_tags['job_code'] = cs_sample_tags.file_name.replace(
    to_replace='[^0-9]', value='', regex=True).astype(int)

cs_sample = pd.merge(left=cs_sample_tags, right=data_full, how='left', on='job_code')
cs_sample['tags'] = cs_sample[['tag_1', 'tag_2', 'tag_3', 'tag_4', 'tag_5', 'tag_6', 'tag_7', 'tag_8', 'tag_9', 'tag_10']].values.tolist()
cs_sample = cs_sample.drop(columns=['tag_1', 'tag_2', 'tag_3', 'tag_4', 'tag_5', 'tag_6', 'tag_7', 'tag_8', 'tag_9', 'tag_10'])
cs_sample.info()

del cs_sample_tags, data_full

"""**# Getting the OCR Outputs**"""

# cs_sample['ocr_output'] = [pytesseract.image_to_string(Image.open(url)) for url in cs_sample.image_drive_url]

# cs_sample['ocr_output']

cs_sample.set_index('job_code', drop=False)

cs_sample_ocr_output = pd.read_csv(folder_path+'data/outputs/cs_sample_ocr_output.csv')
cs_sample_ocr_output = cs_sample_ocr_output.drop(columns=['file_name', 'image_drive_url'])
cs_sample = pd.merge(left=cs_sample, right=cs_sample_ocr_output, how='left', on='job_code')
# cs_sample.to_csv(folder_path+'data/outputs/cs_sample.csv', index=False)
# cs_sample_ocr_output = cs_sample.drop(columns=['functional_area', 'job_description', 'remark', 'job_title', 'start_date', 'expiry_date', 'image_string', 'image_source', 'image_code', 'image_url', 'tags'])
# cs_sample_ocr_output.to_csv(folder_path+'data/outputs/cs_sample_ocr_output.csv', index=False)

# cs_sample = pd.read_csv(folder_path+'data/cs_sample.csv')
cs_sample.info()

def clean_text(text):
  #replaces dashes with my chosen delimiter
  nodash = re.sub('.(-+)', ',', text)
  #strikes multiple periods
  nodots = re.sub('.(\.\.+)', '', nodash)
  #strikes linebreaks
  nobreaks = re.sub('\n', ' ', nodots)
  #strikes extra spaces
  nospaces = re.sub('(  +)', ',', nobreaks)
  #strikes *
  nostar = re.sub('.[*]', '', nospaces)
  #strikes new line and comma at the beginning of the line
  flushleft = re.sub('^\W', '', nostar)
  #getting rid of double commas (i.e. - Evarts)
  comma = re.sub(',{2,3}', ',', flushleft)
  #cleaning up some words that are stuck together (i.e. -  Dawes, Manderson)
  return (comma)

cs_sample['tj_desc'] = [clean_text(text) for text in cs_sample.ocr_output]

cs_sample.info()

cs_sample = cs_sample.rename(columns={'job_code': 'tj_code', 'job_title': 'tj_title', })
cs_sample = cs_sample.drop(columns=['image_drive_url', 'job_description', 'remark', 'functional_area', 'expiry_date', 'image_string', 'image_source', 'image_code', 'image_url'])

"""**# Calculating Job Post Count Disaggregated by Sector**"""

cs_sample['lockdown_status'] = cs_sample.start_date >= '2020-01-01'
cs_sample.groupby(by=['lockdown_status']).size()

cs_sample = cs_sample.drop(columns=['start_date'])

"""**# Calculating Job Post Count Disaggregated by Job Title**"""

# onet_titles = [title.split() for title in onet_data.onet_title] #list of lists

# onet_alt_titles = [(' '.join(titles)).split() for titles in onet_data.onet_title_alt] #list of lists

# onet_techs = [(' '.join(techs)).split() for techs in onet_data.onet_tech] #list of lists

# onet_descs = [desc.split() for desc in onet_data.onet_desc] #list of lists

# len(onet_titles), len(onet_alt_titles), len(onet_techs), len(onet_descs)

# onet_corpus = pd.DataFrame([list(map(lambda a, b, c, d:a + b + c + d, onet_titles, onet_alt_titles, onet_techs, onet_descs))]).T

# onet_corpus[0][0]

"""**# Creating Relevant Corpora**"""

# onet_corpus = onet_data.onet_title + ' ' + \
#     [' '.join(titles) for titles in onet_data.onet_title_alt] + \
#     [' '.join(techs) for techs in onet_data.onet_tech] #+ \
#     # ' ' + onet_data.onet_desc

onet_data['onet_family'] = onet_data['onet_code'].str.slice(stop=2)

onet_corpus = onet_data.onet_title + ' ' + \
    [' '.join(titles) for titles in onet_data.onet_title_alt]

# onet_data['corpus_content'] = onet_data.onet_title + ' ' + \
#     [' '.join(titles) for titles in onet_data.onet_title_alt] #+ \
    # [' '.join(techs) for techs in onet_data.onet_tech] #+ \
    # ' ' + onet_data.onet_desc

# onet_corpus = onet_data.groupby('onet_family')['corpus_content'].apply(lambda x: ' '.join(x)).reset_index()

tj_corpus_title = [unquote(str(title)) for title in tj_data.tj_title]
tj_corpus_title = [re.sub('\+', ' ', title) for title in tj_corpus_title]
tj_corpus_desc = tj_data.tj_desc

cs_sample_title = [unquote(str(title)) for title in cs_sample.tj_title]
cs_sample_title = [re.sub('\+', ' ', title) for title in cs_sample_title]
cs_sample_desc = cs_sample.tj_desc

tj_data.to_csv(folder_path+'data/outputs/tj_data.csv', index=False)
onet_corpus.to_csv(folder_path+'data/outputs/onet_corpus.csv', index=False)

onet_corpus

"""**# Fitting the tf-idf Vectorizer on the Reference Corpus**"""

tfidf_vect = TfidfVectorizer(tokenizer=nltk_tokenizer, stop_words='english')
onet_tfidf = tfidf_vect.fit_transform(onet_corpus)

# tfidf_vect.to_csv()
with open(folder_path+'data/outputs/tfidf_vect.txt', 'w') as f:
    f.write(str(onet_tfidf))

onet_tfidf

"""**# Vectorizing Titles and Descriptions Separately**"""

# cs_sample_title.shape

cs_sample_tfidf_title = tfidf_vect.transform(cs_sample_title)
cs_sample_tfidf_desc = tfidf_vect.transform(cs_sample_desc)

"""**Transforming the CS Sample**"""

# onet_data.onet_family.unique()

"""**# Calculating Cosine Similarity with Different Weights**"""

wl_title = 0.6
we_title = 1
wl_desc = 1-wl_title
we_desc = 1

cs_sample_title = linear_kernel(cs_sample_tfidf_title, onet_tfidf)
cs_sample_desc = linear_kernel(cs_sample_tfidf_desc, onet_tfidf)
cs_sample_comb = pd.DataFrame(
    data=(cs_sample_title**we_title)*wl_title + (cs_sample_desc**we_desc)*wl_desc,
    columns=onet_data.onet_code,#onet_data.onet_family.unique(),
    index=cs_sample.tj_code)

cs_sample_comb.to_csv(folder_path+'data/outputs/cs_sample_comb.csv', index=False)

del wl_title, we_title, wl_desc, we_desc

"""**# Finding the Job Family with the Highest Average Score**"""

# matches_family = pd.DataFrame()
# for family in set([col_name[0:2] for col_name in onet_data.onet_code]):
#     matches_family[family] = np.mean(cs_sample_comb.filter(
#         regex=r'^' + re.escape(family)), axis=1)

# matches_family['match'] = matches_family.idxmax(axis=1)

# matches_family.to_csv(folder_path+'data/outputs/cs_sample_matches_family.csv', index=False)

# del family

"""**# Finding the Job with the Highest Score in that Family**"""

cs_sample_comb.info()

# cs_sample.loc(660809)
# print(cs_sample.loc[[660809]])
# cs_sample=cs_sample.set_index('tj_code', drop=False)
# print(cs_sample_comb.loc[[660809]])
# cs_sample_comb['tj_code']
# cs_sample['tj_code'] = cs_sample_tags['job_code']
cs_sample.info()

matches = pd.DataFrame(index=cs_sample.tj_code)
for job in cs_sample.tj_code:
  # try:
    # family = matches_family.match[job]
    # print(job)
    # code = cs_sample_comb.loc[job, cs_sample_comb.columns.str.startswith(family)].idxmax()
    # matches.loc[job, 'onet_code'] = code

  code = cs_sample_comb.loc[job, cs_sample_comb.columns].idxmax() #.str.startswith(family)].idxmax()
  family = code[0:2]
  matches.loc[job, 'onet_code'] = code
  matches.loc[job, 'onet_family'] = family

  # family = cs_sample_comb.loc[job, cs_sample_comb.columns].idxmax() #.str.startswith(family)].idxmax()
  # # print(family)
  # matches.loc[job, 'onet_family'] = family
  
  # except:
  #   print (job)
  #   continue

matches = matches.reset_index()

# del family, job, code

matches = matches[matches['onet_family'].notna()]
matches.info()

"""**# Storing Matches in an Easy-to-Read Format**"""

import numpy as np
matches = pd.merge(
    left=matches,
    right=cs_sample[['tj_code', 'tj_title', 'tj_desc', 'tags', 'lockdown_status']],
    on='tj_code')
matches = pd.merge(
    left=matches,
    right=onet_data[['onet_code', 'onet_title', 'onet_desc']],
    on='onet_code')
matches.tj_title = [unquote(str(title)) for title in matches.tj_title]
matches.tj_title = [re.sub('\+', ' ', title) for title in matches.tj_title]

# matches_family.groupby('match').size()
matches_title = matches.groupby('onet_family').size()

matches = matches.set_index('tj_code', drop=False)

matches.info()

for job in matches.tj_code:
  # try:
  tags = matches.at[job,'tags']
  # tags = [x for x in  list(set(tags)) if pd.notnull(x)]
  tag_families = [int(str(tag)[0:2]) for tag in tags if pd.notnull(tag)]
  tag_families = (tag_families + [None]*10)[:10]
  # onet_code = matches.at[job,'onet_code']
  onet_family = matches.at[job,'onet_family']
  # print (tags, onet_code)
  print (onet_family, tag_families, int(onet_family) in tag_families)

  matches.loc[job, 'tag_families'] = [[tag_families]]
  matches.loc[job, 'match_value'] = [int(onet_family) in tag_families]

  # code = cs_sample_comb.loc[job, cs_sample_comb.columns.str.startswith(family)].idxmax()
  # matches.loc[job, 'onet_code'] = code
  # except:
  #   # print (job)
  #   continue

matches.to_csv(folder_path+'data/outputs/cs_sample_matches.csv', index=False)

matches.match_value

matches.groupby(['lockdown_status', 'onet_family']).size()

matches.onet_code.nunique()

matches.onet_family.nunique()

tag_list = matches.tags.apply(pd.Series).stack().reset_index(drop=False)

tag_family_list = matches.tag_families.explode('tag_families')
tag_family_list = tag_family_list.explode('tag_families')

tag_list[0].nunique()

tag_family_list.nunique()

tag_family_list

tag_list[0]

matches.onet_code